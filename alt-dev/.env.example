# QwenAssistant Environment Configuration
# Copy this to .env and customize for your environment

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# Database URL for persistent storage
# SQLite: sqlite:///./data/local_ai.db (local file)
# PostgreSQL: postgresql://user:password@localhost/qwen_db
DATABASE_URL=sqlite:///./data/local_ai.db

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Path to AI models directory
MODEL_PATH=../models

# Hardware profile for model selection
# Options: light, medium, heavy, auto (default: auto)
HARDWARE_PROFILE=auto

# Maximum tokens for model responses (default: 256)
MAX_TOKENS=256

# Model temperature for sampling (0.0-1.0, default: 0.7)
MODEL_TEMPERATURE=0.7

# ============================================================================
# VOICE CONFIGURATION
# ============================================================================

# Enable voice input/output
VOICE_ENABLED=true

# Sample rate for audio processing (Hz, default: 16000)
VOICE_SAMPLE_RATE=16000

# Voice mode (continuous, push_to_talk, auto)
VOICE_MODE=push_to_talk

# ============================================================================
# PRIVACY & SECURITY
# ============================================================================

# Enable encryption at rest for sensitive data
PRIVACY_ENCRYPT_AT_REST=true

# Secret key for encryption (change in production!)
SECRET_KEY=your-secret-key-change-in-production

# Salt for privacy operations (change in production!)
PRIVACY_SALT=your-privacy-salt-change-in-production

# Data retention policy
# Options: permanent, session (auto-delete on exit), days:N (delete after N days)
DATA_RETENTION=permanent

# ============================================================================
# CACHING
# ============================================================================

# Redis URL for caching (optional)
# Leave empty to disable caching
# Format: redis://localhost:6379/0
REDIS_URL=redis://localhost:6379/0

# Cache TTL in seconds (default: 3600)
CACHE_TTL=3600

# ============================================================================
# LOGGING
# ============================================================================

# Log level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format
# Options: text, json
LOG_FORMAT=text

# Log file path (optional, leave empty for console only)
LOG_FILE=./logs/qwen.log

# ============================================================================
# API CONFIGURATION
# ============================================================================

# API host and port
API_HOST=0.0.0.0
API_PORT=8000

# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# Enable API documentation
ENABLE_DOCS=true

# ============================================================================
# OPTIONAL: CLOUD INTEGRATION
# ============================================================================

# Enable cloud features (search, LLM escalation, sync)
# Leave empty or false to disable cloud features
CLOUD_ENABLED=false

# OpenAI API key (for GPT models as fallback)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Groq API key (for fast LLM inference)
# Get from: https://console.groq.com
GROQ_API_KEY=

# Google Search API key (for web search)
# Get from: https://console.cloud.google.com
GOOGLE_SEARCH_API_KEY=

# Web search engine selection
# Options: google, bing, duckduckgo, tavily
WEB_SEARCH_ENGINE=duckduckgo

# ============================================================================
# OPTIONAL: SUPABASE (For cloud backup/sync)
# ============================================================================

# Supabase project URL
SUPABASE_URL=

# Supabase anonymous key
SUPABASE_ANON_KEY=

# Enable Supabase integration
ENABLE_SUPABASE=false

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Number of workers (for production)
WORKERS=1

# Queue size for processing
QUEUE_SIZE=100

# Vector search results limit
VECTOR_SEARCH_K=5

# Semantic search similarity threshold (0.0-1.0)
SIMILARITY_THRESHOLD=0.5

# ============================================================================
# DEVELOPMENT
# ============================================================================

# Enable debug mode
DEBUG=false

# Enable auto-reload on file changes (development only)
RELOAD=true

# Environment type
# Options: development, production
ENV=development

# ============================================================================
# HARDWARE ACCELERATION
# ============================================================================

# Enable CUDA GPU acceleration (NVIDIA)
CUDA_ENABLED=true

# Enable Metal GPU acceleration (macOS)
METAL_ENABLED=true

# Enable OpenCL (cross-platform)
OPENCL_ENABLED=false

# Number of GPU layers to offload (0 to disable)
GPU_LAYERS=40

# ============================================================================
# MONITORING
# ============================================================================

# Enable performance monitoring
MONITORING_ENABLED=false

# Metrics export interval (seconds)
METRICS_INTERVAL=60

# Budget limits
BUDGET_MONTHLY_LIMIT=100.00
BUDGET_DAILY_LIMIT=10.00

# ============================================================================
# ADVANCED
# ============================================================================

# Number of threads for model inference
NUM_THREADS=4

# Context window size (tokens)
CONTEXT_WINDOW=2048

# Memory limit for models (GB)
MEMORY_LIMIT=8

# Embedding dimension
EMBEDDING_DIM=768

# ============================================================================
# NOTES
# ============================================================================
#
# 1. Development vs Production:
#    - Set ENV=development for local testing with DEBUG=true
#    - Set ENV=production for deployment with DEBUG=false
#
# 2. Model profiles:
#    - light: Best for 8GB RAM systems (fast, limited capability)
#    - medium: Recommended for 16GB RAM (balanced)
#    - heavy: For 32GB+ RAM (best quality, slower)
#    - auto: Automatic detection based on available hardware
#
# 3. Security:
#    - ALWAYS change SECRET_KEY and PRIVACY_SALT in production
#    - Keep API keys secure and never commit to version control
#    - Use environment variables for sensitive data
#
# 4. Performance:
#    - Enable GPU acceleration if available (CUDA, Metal, or OpenCL)
#    - Increase GPU_LAYERS for better performance on powerful GPUs
#    - Adjust NUM_THREADS based on CPU cores
#
# 5. Cloud Integration:
#    - Optional: Use OPENAI_API_KEY for GPT model fallback
#    - Optional: Use GROQ_API_KEY for Groq LLM access
#    - Optional: Enable Supabase for cloud backup and sync
#
# 6. Memory Management:
#    - MEMORY_LIMIT controls how much RAM models can use
#    - Set based on available system RAM minus OS/apps overhead
#    - Monitor with: nvidia-smi (GPU) or htop (CPU)
#
# ============================================================================
