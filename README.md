# Local AI Assistant

A privacy-focused, local-first AI assistant that runs entirely on your hardware with no cloud dependencies.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey)](#)

## ğŸš€ Overview

The Local AI Assistant is a cutting-edge AI application that provides chat, voice interaction, and reasoning capabilities while ensuring all data processing occurs locally on your device. Built with privacy as the core principle, it offers a compelling alternative to cloud-based AI assistants.

## ğŸŒŸ Key Features

### ğŸ”’ Privacy First
- **Local Processing**: All AI inference happens on your device
- **End-to-End Encryption**: Conversation data encrypted at rest
- **No Cloud Dependencies**: Zero external data transmission
- **Data Classification**: Automatic sensitive data detection

### ğŸ–¥ï¸ Hardware Adaptive
- **Automatic Detection**: CPU/GPU/NPU capability detection
- **Dynamic Model Selection**: Optimizes for your hardware
- **Three Performance Tiers**: 
  - Light (CPU-only): 3-7B parameter models
  - Medium (GPU/NPU): 7-13B parameter models
  - Heavy (High-end GPU): 13B+ parameter models

### ğŸ™ï¸ Voice Interaction
- **Wake Word Detection**: Hands-free activation ("Hey assistant")
- **Speech-to-Text**: Accurate voice recognition with Whisper
- **Text-to-Speech**: Natural voice responses with Piper
- **Real-time Processing**: Low-latency voice interaction

### ğŸ’¬ Rich Chat Interface
- **Streaming Responses**: Real-time message updates
- **Conversation History**: Persistent chat storage
- **Multi-modal Input**: Voice and text support
- **Responsive Design**: Works on all screen sizes

### ğŸŒ Cross-Platform
- **Windows**: Native desktop application
- **macOS**: Native desktop application
- **Linux**: Native desktop application
- **Consistent Experience**: Same features across platforms

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Desktop Application                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Voice     â”‚  â”‚   Chat UI   â”‚  â”‚  Hardware Detection â”‚  â”‚
â”‚  â”‚ Interface   â”‚  â”‚             â”‚  â”‚                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend Services                         â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Model     â”‚  â”‚  Memory     â”‚  â”‚   Privacy &         â”‚  â”‚
â”‚  â”‚  Routing    â”‚  â”‚  Storage    â”‚  â”‚  Security           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AI Model Execution                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   LLM       â”‚  â”‚   Voice     â”‚  â”‚  Hardware-          â”‚  â”‚
â”‚  â”‚  Inference  â”‚  â”‚  Models     â”‚  â”‚  Optimized          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Execution          â”‚  â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

### Backend
- **Framework**: [FastAPI](https://fastapi.tiangolo.com/) (Python)
- **Database**: [SQLite](https://www.sqlite.org/) with [SQLModel](https://sqlmodel.tiangolo.com/)
- **AI Inference**: [ONNX Runtime](https://onnxruntime.ai/), [llama.cpp](https://github.com/ggerganov/llama.cpp)
- **Containerization**: [Docker](https://www.docker.com/)

### Frontend
- **Desktop Framework**: [Tauri](https://tauri.app/) (Rust + React)
- **UI Library**: [React](https://reactjs.org/) with [Tailwind CSS](https://tailwindcss.com/)
- **State Management**: [Zustand](https://github.com/pmndrs/zustand)
- **Voice Processing**: Web Audio API

### AI Models
- **Chat Models**: Llama 3.2, Mistral 7B, Llama 3.3
- **Voice Models**: Whisper (STT), Piper (TTS)
- **Wake Word**: Porcupine
- **Model Format**: GGUF for optimal local performance

## ğŸ“ Project Structure

```
qwenassistant/
â”œâ”€â”€ backend/              # FastAPI backend services
â”‚   â”œâ”€â”€ app/              # Main application code
â”‚   â”‚   â”œâ”€â”€ api/          # API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ v1/       # API version 1
â”‚   â”‚   â”‚       â”œâ”€â”€ endpoints/  # Individual endpoint handlers
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ hardware.py
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ memory.py
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ privacy.py
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ voice.py
â”‚   â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core/         # Core application logic
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ models/       # Data models
â”‚   â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic services
â”‚   â”‚   â”‚   â”œâ”€â”€ hardware_detector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model_router.py
â”‚   â”‚   â”‚   â”œâ”€â”€ privacy_service.py
â”‚   â”‚   â”‚   â””â”€â”€ voice_service.py
â”‚   â”‚   â””â”€â”€ main.py       # Application entry point
â”‚   â”œâ”€â”€ tests/            # Unit and integration tests
â”‚   â”œâ”€â”€ Dockerfile        # Backend Docker image
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ frontend/             # Tauri desktop application
â”‚   â”œâ”€â”€ src/              # React application source
â”‚   â”‚   â”œâ”€â”€ components/   # React components
â”‚   â”‚   â”‚   â””â”€â”€ ChatInterface.tsx
â”‚   â”‚   â”œâ”€â”€ services/     # Frontend services
â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
â”‚   â”‚   â”‚   â””â”€â”€ voiceService.ts
â”‚   â”‚   â”œâ”€â”€ App.tsx       # Main app component
â”‚   â”‚   â”œâ”€â”€ index.css     # Global styles
â”‚   â”‚   â””â”€â”€ main.tsx      # Entry point
â”‚   â”œâ”€â”€ src-tauri/        # Tauri backend code (Rust)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â””â”€â”€ main.rs
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ tauri.conf.json
â”‚   â”œâ”€â”€ package.json      # Frontend dependencies
â”‚   â””â”€â”€ index.html        # HTML entry point
â”œâ”€â”€ docker-compose.yml    # Service orchestration
â”œâ”€â”€ Makefile              # Development commands
â”œâ”€â”€ README.md             # Project documentation
â”œâ”€â”€ LICENSE               # License information
â”œâ”€â”€ INSTALLATION.md       # Installation guide
â”œâ”€â”€ DEPLOYMENT.md         # Deployment guide
â”œâ”€â”€ TESTING.md            # Testing and optimization plan
â”œâ”€â”€ PROJECT_PLAN.md       # Comprehensive project plan
â””â”€â”€ SUMMARY.md            # Project summary
```

## ğŸš€ Quick Start

### Prerequisites
- Docker Desktop
- Node.js 18+
- Rust (for Tauri)
- Python 3.11+

### Development Setup

1. **Clone the repository**:
```bash
git clone https://github.com/bentman/JARVISv2.git
cd JARVISv2/qwenassistant
```

2. **Start backend services**:
```bash
cd backend
docker-compose up -d
```

3. **Install frontend dependencies**:
```bash
cd ../frontend
npm install
```

4. **Start frontend application**:
```bash
npm run tauri dev
```

### Production Build

1. **Build backend Docker image**:
```bash
cd backend
docker build -t local-ai-assistant-backend .
```

2. **Build frontend desktop application**:
```bash
cd frontend
npm run tauri build
```

## ğŸ¯ Core Components

### Hardware Detection Service
Automatically detects CPU, GPU, and NPU capabilities to select the optimal model profile:
- **Light Profile**: CPU-only systems with 3-7B parameter models
- **Medium Profile**: GPU/NPU systems with 7-13B parameter models
- **Heavy Profile**: High-end GPU systems with 13B+ parameter models

### Model Routing System
Intelligently routes requests to the appropriate local AI model based on:
- Hardware profile
- Task type (chat, coding, reasoning)
- Resource availability
- Performance requirements

### Voice Interaction
Complete voice interface with:
- Wake word detection using Porcupine
- Speech-to-text conversion with Whisper
- Text-to-speech synthesis with Piper
- Hands-free operation

### Privacy & Security
- End-to-end encryption for all stored data
- Local processing by default with no cloud dependencies
- Data classification for sensitive information
- Secure communication between components

### Memory Management
- Persistent conversation history with SQLite
- Encrypted storage for sensitive data
- Automatic cleanup based on retention policies
- Efficient database queries and indexing

## ğŸ“– Documentation

- [qwenassistant/INSTALLATION.md](INSTALLATION.md) - Detailed installation instructions
- [qwenassistant/DEPLOYMENT.md](DEPLOYMENT.md) - Deployment and configuration guide
- [qwenassistant/TESTING.md](TESTING.md) - Testing strategies and optimization plan
- [qwenassistant/PROJECT_PLAN.md](PROJECT_PLAN.md) - Comprehensive project plan
- [qwenassistant/SUMMARY.md](SUMMARY.md) - Project summary and implementation details

## ğŸ§ª Testing

The project includes comprehensive testing strategies:

- **Unit Testing**: pytest for backend, Jest for frontend
- **Integration Testing**: End-to-end API and component testing
- **Performance Testing**: Load and stress testing
- **Security Testing**: Vulnerability scanning and penetration testing

### Test Commands
```bash
# Run all tests
make test

# Run backend tests
make test-backend

# Run frontend tests
make test-frontend
```

## ğŸ› ï¸ Development Commands

```bash
# Install all dependencies
make setup

# Start backend in development mode
make backend-dev

# Start frontend in development mode
make frontend-dev

# Start both backend and frontend
make dev

# Build backend Docker image
make backend-build

# Build frontend desktop application
make frontend-build

# Build both backend and frontend
make build

# Clean build artifacts
make clean

# Show help
make help
```

## ğŸš€ Future Enhancements

### Short-term Goals
1. Model fine-tuning with LoRA
2. Plugin architecture for extensions
3. Advanced RAG implementation
4. Multi-user support

### Long-term Vision
1. Federated learning capabilities
2. Edge computing extensions
3. Enterprise management features
4. Mobile platform support

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines for details.

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com/) - Modern, fast web framework
- [Tauri](https://tauri.app/) - Build smaller, faster, and more secure desktop applications
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Inference of LLMs in pure C/C++
- [Whisper](https://github.com/openai/whisper) - Robust speech recognition model
- [Piper](https://github.com/rhasspy/piper) - Fast neural text to speech system
- [Porcupine](https://github.com/Picovoice/porcupine) - Wake word detection engine

## ğŸ“ Support

For support, please open an issue on GitHub or contact us at support@local-ai-assistant.com.